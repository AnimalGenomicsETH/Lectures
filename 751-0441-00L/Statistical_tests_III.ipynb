{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588c2112-d568-43dd-ba9a-edf1b8af825a",
   "metadata": {},
   "source": [
    "# Statistical Tests III\n",
    "---\n",
    "\n",
    "Solutions are provided below.  \n",
    "Each example contains easier parts at the start, plus more challenging extensions.  \n",
    "The extensions are useful to understand the concepts more generally, and are likely close to the difficulty of the final parts of an exam question.\n",
    "\n",
    "---\n",
    "#### Example 1\n",
    "\n",
    "Let's look now at some example data for average annual temperature.\n",
    " - Make a scatterplot of average temperature over time. \\\n",
    "   Considering the spread of data, which correlation test might be most appropriate?\n",
    " - Calculate the other type of correlation as well, and compare the significances. \\\n",
    "   How does that match the interpretation for linear versus monotonic relationships?\n",
    " \n",
    "**Extension**\n",
    " - What happens if you change the year values slightly (by less than 1.0 years)? \\\n",
    "   How much do the correlations change? \\\n",
    "   What if you change by more than 1.0 years?\n",
    " - What happens to the correlations if you change one temperature value to be a huge outlier?\n",
    " - There is another correlation metric implemented in R called [Kendall rank correlation](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient). \\\n",
    "   See how the correlation coefficient for this test compares to Pearson and Spearman correlations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4787a8-fd24-49dc-9358-be6c0eeb5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The years have been relabelled for simplicitly to start from 1.\n",
    "Temperatures <- c(51.5,52.0,52.5,52.7,48.6,52.3,49.6,50.8,51.0,52.8,52.0,52.6,53.0,52.9,51.4,50.8,51.2,50.3,51.0,50.4,51.6,50.6,49.7,51.0,53.9,53.5,52.1,50.6,51.8,51.7,51.2,52.4,50.1,53.6,50.3,54.7,53.9,54.3,53.4,52.9,53.3,53.7,53.8,52.0,55.0,52.1,53.4,53.8,53.8,51.9,52.1,52.7,51.8,56.6,53.3,55.6,56.3,56.2,56.1,56.2,53.6,55.7,56.3)\n",
    "Years <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63)\n",
    "\n",
    "# Plot the data.\n",
    "plot(Years, Temperatures, type = \"p\",col=\"darkred\",cex=3,lwd=4)\n",
    "\n",
    "# Since the data has no obvious outliers, we can calculate the Pearson correlation, which is the default .\n",
    "cor.test(Years,Temperatures)\n",
    "\n",
    "# We'll also calculate the Spearman correlation.\n",
    "# We have \"ties\" in our data, where the temperatures are the same across two years, so we have to use an approximation to handle these ties rather than the exact method.\n",
    "cor.test(Years,Temperatures,method='spearman',exact=F)\n",
    "\n",
    "# The data is quite linear, but has a lot of minor fluctuations that disrupt the monotonic relationship, so the Pearson correlation is larger than the Spearman correlation (but both are significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd9fd0-aeb7-42f4-ac3a-7243150ad781",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1 extension\n",
    "# What happens if you change year values slightly, how much do the correlations change?\n",
    "\n",
    "# We can add a small amount of normally distributed random noise (mean=0 and sd=0.2) to Years as Years+rnorm(length(Years),0,0.2)\n",
    "Years_noise <- Years+rnorm(length(Years),0,0.2)\n",
    "plot(Years_noise, Temperatures, type = \"p\",col=\"darkred\",cex=3,lwd=4)\n",
    "\n",
    "# We can recalculate the correlations.\n",
    "cor.test(Years_noise,Temperatures,method=\"pearson\",exact=F)\n",
    "cor.test(Years_noise,Temperatures,method=\"spearman\",exact=F)\n",
    "\n",
    "# There is a slight but insignificant change to the correlation and p value when using pearson R.\n",
    "# Since the years don't change enough to swap ranks, the spearman R is totally unchanged.\n",
    "# Increasing the variance (try rerunning with Years+rnorm(length(Years),0,5)) will affect both correlations.\n",
    "\n",
    "# What happens to the correlations if you change one value to be a huge outlier?\n",
    "Temperatures_noise = Temperatures\n",
    "Temperatures_noise[15] = 80.7\n",
    "\n",
    "cor.test(Years_noise,Temperatures_noise,method=\"pearson\",exact=F)\n",
    "cor.test(Years_noise,Temperatures_noise,method=\"spearman\",exact=F)\n",
    "\n",
    "# Pearson correlation can be totally changed (depending on how big the outlier is), but there will be a much smaller effect for Spearman, as the rank of the datapoint matters, not the exact value.\n",
    "\n",
    "# We can also calculate the Kendall correlation, which measures how \"diagonal\" data is.\n",
    "# Althought the algorithm method is very different to Pearson or Spearman, the correlation coefficient is fairly similar.\n",
    "cor.test(Years_noise,Temperatures_noise,method='kendall',exact=F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e92ada3-7a43-4394-8989-ed46910342e2",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 2\n",
    "\n",
    "Let's investigate the \"spurious correlation of ratios\" effect.\n",
    " - Generate three sets of normally distributed data, which can all have the same mean and std.\n",
    " - Make a scatterplot using two of the datasets as X verus Y and the third dataset as the colour of each point.\n",
    " - Test the Pearson correlation of X and Y, and then test the Pearson correlation of X/Z and Y/Z.\n",
    "\n",
    "**Extension**\n",
    " - What happens if you change the standard deviation for X/Y/Z? \\\n",
    "   Does the correlation match the equation from the slides?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50711f6d-1c94-4e87-8236-c4bfd0853263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 3 random variables that are normally distributed (with identical parameters).\n",
    "N_samples = 100\n",
    "mean = 50\n",
    "\n",
    "X <- rnorm(N_samples,mean,5)\n",
    "Y <- rnorm(N_samples,mean,5)\n",
    "Z <- rnorm(N_samples,mean,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0fd1e-a9d5-4cec-9aed-da5784d8e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a colour palette (\"viridis\"-like) that goes from blue->yellow.\n",
    "YlOrBrRdBu <- c(\"#FDE725\", \"#21908C\", \"#3B528B\", \"#440154\")\n",
    "col <- colorRampPalette(YlOrBrRdBu)(50)\n",
    "\n",
    "# Roughly bin the Z values so we can colour each scatter point by the Z value, and then plot.\n",
    "cols = col[cut(Z,50)]\n",
    "plot(X, Y, col=cols,type = \"p\",cex=3,lwd=4)\n",
    "\n",
    "# Let's calculate the correlation as well.\n",
    "# This can be significant by chance, but it is unlikely.\n",
    "cor.test(X,Y)\n",
    "\n",
    "# Now let's divide X & Y by Z.\n",
    "# This may be some normalisation procedure or similar approach.\n",
    "# We are using the same colours as determined before, just now plotting X/Z versus Y/Z.\n",
    "plot(X/Z, Y/Z, col=cols,type = \"p\",cex=3,lwd=4)\n",
    "\n",
    "# And calculate the new correlation.\n",
    "# This should be significant now, but only due to the \"spurious correlation of ratios\" effect.\n",
    "cor.test(X/Z,Y/Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efec728-97fc-474d-9c7f-907792265004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2 extension\n",
    "# What happens if you alter the standard deviation for X/Y/Z?\n",
    "\n",
    "# If the standard deviation is huge the result is less significant, while a lower deviation is more reliably significant.\n",
    "# Huge outliers with high deviation can skew the data much more, so this is expected.\n",
    "\n",
    "# From the equation from the slides, we expect a correlation of roughly 0.5 if the variances are equal.\n",
    "\n",
    "# Now let's make the variance in X and Y much greater than in Z, and we should see a low correlation value.\n",
    "X <- rnorm(N_samples,mean,20)\n",
    "Y <- rnorm(N_samples,mean,15)\n",
    "Z <- rnorm(N_samples,mean,2)\n",
    "cor.test(X/Z,Y/Z)\n",
    "\n",
    "# On the other hand, if we increase the variance in Z to be large, the correlation should approach 1. \n",
    "X <- rnorm(N_samples,mean,2)\n",
    "Y <- rnorm(N_samples,mean,3)\n",
    "Z <- rnorm(N_samples,mean,20)\n",
    "cor.test(X/Z,Y/Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073567b-6c1b-49c6-9754-be3a21cc51c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### Example 3\n",
    "\n",
    "We can also examine correlations across many sets of data at once, rather than between just a pair of observations.\n",
    " - Create a 10x10 matrix of randomly distributed variates.\n",
    " - Calculate the correlation coefficient and signifiance across all the pairwise combinations.\n",
    " - Plot the results using the `corrplot` package.\n",
    "\n",
    "**Extension**\n",
    " - Verify that the correlation matrix values match calculating the correlations between some pairs of columns.\n",
    " - If we change the distribution from exponential to normal, how many correlations are \"falsely\" significant?\n",
    " - If we change the correlation to spearman (from pearson), would we expect to see more significant correlations with the exponential distribution?\n",
    " - Consider how many of these \"random\" sets of data are significantly correlated. \\\n",
    "   If we account for \"multiple hypothesis testing\", how many would we expect by chance?\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238d9df-2e52-4c7d-806d-eae6cfdebc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the corrplot package if necessary.\n",
    "#install.packages(\"corrplot\")\n",
    "\n",
    "# Load the corrplot package.\n",
    "library(corrplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d674b3-3e15-4ada-b4eb-8de75390e49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a matrix with random expoential values, calculate the (pairwise) correlation matrix.\n",
    "A <- matrix(rexp(100,5),nrow=10)\n",
    "\n",
    "# Add some names to the columns to make it look a bit nicer.\n",
    "colnames(A) <- c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\")\n",
    "\n",
    "# Test for correlations across the matrix.\n",
    "correlations <- cor(A,method=\"pearson\")\n",
    "\n",
    "# Also calculate the p-values for the correlations.\n",
    "p = cor.mtest(A)$p\n",
    "\n",
    "# Plot the correlation matrix, indicate which correlations are not significant.\n",
    "corrplot(correlations,type = \"upper\",p.mat=p,sig.level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067657ce-1f78-474d-a014-938088facd18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 3 extension\n",
    "# What happens if you alter the standard deviation for X/Y/Z?\n",
    "\n",
    "# Verify that the correlation matrix values match calculating the correlations between each column.\n",
    "# We can access each column like A[,\"Alpha\"] or A[,\"Golf\"], and each element of the correlation matrix like correlations[\"Alpha\",\"Golf\"].\n",
    "cor.test(A[,\"A\"],A[,\"G\"])\n",
    "correlations[\"A\",\"G\"]\n",
    "\n",
    "# If we change the distribution from exponential to normal, how many correlations are \"falsely\" significant?\n",
    "A <- matrix(rnorm(100,50,2),nrow=10)\n",
    "colnames(A) <- c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\")\n",
    "correlations <- cor(A,method=\"pearson\")\n",
    "p = cor.mtest(A)$p\n",
    "corrplot(correlations,type = \"upper\",p.mat=p,sig.level=0.05)\n",
    "\n",
    "# If we change the correlation to spearman (from pearson), would we expect to see more significant correlations with the exponential distribution?\n",
    "A <- matrix(rexp(100),nrow=10)\n",
    "colnames(A) <- c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\")\n",
    "correlations <- cor(A,method=\"spearman\")\n",
    "p = cor.mtest(A)$p\n",
    "corrplot(correlations,type = \"upper\",p.mat=p,sig.level=0.05)\n",
    "\n",
    "# An exponential distribution generally has more extreme values (outliers) than a normal distribution, which could either lead to a larger or smaller correlation.\n",
    "# Since spearman is more robust to outliers (and we don't expect any signficance from a random set of values), we would expect it to have fewer significant correlations compared to pearson.\n",
    "\n",
    "\n",
    "# We conducted n(n-1)/2 tests, so for n=10, that is 45 tests.\n",
    "# Given a false positive rate of approximately 5% (p=0.05), we would estimate around 2 to 3 tests to be significant.\n",
    "\n",
    "# We can calculate this from the \"p\" variable, removing the 10 diagonal tests (data always correlates with itself perfectly) and dividing by two (we double test A correlating with B and B correlating with A).\n",
    "(sum(p<0.05)-10)/2\n",
    "\n",
    "# Or a bit more fancy, we can consider only the \"upper triangle\" of the matrix, which removes the diagonal elements and the duplicate tests above/below the diagonal.\n",
    "sum(p[upper.tri(p)]<0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bc34e-278e-4bd1-bdd8-62a69415931e",
   "metadata": {},
   "source": [
    "---\n",
    "## Real world example\n",
    "\n",
    "This is a slightly more realistic case, where we are examining some [actual data](https://opendata.swiss/en/dataset/rinder-verteilung-pro-gemeinde).  \n",
    "The goal here is to manipulate the data into a useful shape, and then assess statistical significance.  \n",
    "These examples are harder than you would encounter in an exam and use more advanced data analysis techniques, but are useful to try.\n",
    "\n",
    "This data is the number of cattle for each commune in Switzerland\n",
    " - Filter out communes that have values of 0 for the count of cattle, as these are likely incomplete records.\n",
    " - Plot the count of cattle against the countPerSurfacekm2 (and also against countPer100Inhabitants).\n",
    " - Test for correlation betweeen these variables\n",
    "   -  Does this match your interpretation about which types communes (in terms of land area or population density) would have large amounts of cattle?\n",
    " - Try different levels of filtering (N=100, 1000, etc). \\\n",
    "   How does that impact the results/correlations? \\\n",
    "   In particular, notice the correlation for the threhsold N=750 and how this relatives to correlation transitivity.\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec1223-17b5-4a67-9130-8efeaf13f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the csv, which uses ';' as the separator rather than ','.\n",
    "# The first row of the file is also some download information rather than data, so we skip the first row.\n",
    "cattle_by_communes <- read.csv('cattle-map-commune.csv',sep=';',skip=1)\n",
    "\n",
    "# Print out the first few rows so we can see what the data looks like.\n",
    "head(cattle_by_communes)\n",
    "\n",
    "# Filter out communes that have less than N cattle.\n",
    "N=1\n",
    "cattle_by_communes <- cattle_by_communes[(cattle_by_communes$count >= N),] \n",
    "\n",
    "# Plot the data.\n",
    "plot(countPerSurfacekm2 ~ count,data=cattle_by_communes)\n",
    "\n",
    "# And test for Spearman correlations.\n",
    "cor.test(~ countPerSurfacekm2 + count,data=cattle_by_communes,method=\"spearman\",exact=FALSE)\n",
    "cor.test(~ countPer100Inhabitants + count,data=cattle_by_communes,method=\"spearman\",exact=FALSE)\n",
    "cor.test(~ countPer100Inhabitants + countPerSurfacekm2,data=cattle_by_communes,method=\"spearman\",exact=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf294f-2297-4d12-a1c0-aa39638a30d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional example 1\n",
    "\n",
    "\n",
    "We discussed the datasaurus dozen in the lecture, where obviously different patterns end up with incredibly similar statistical properties.  \n",
    " - Plot out some the different datasets and calculate the correlation coefficients. How does the coefficient compare to what you would estimate.\n",
    " - How do the values compare across different datasets?\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d13977-47ff-47d2-9013-906225f16f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load the datasaurus package, which contains the data for the \"data dozen\" set.\n",
    "#install.packages(\"datasauRus\")\n",
    "library(datasauRus)\n",
    "\n",
    "# \"Slice\" out the points for the 'dino' dataset.\n",
    "# We'll plot them and then calculate the correlation coefficient.\n",
    "dino <- datasaurus_dozen[datasaurus_dozen$dataset == 'dino',]\n",
    "plot(y ~ x, data=dino, type = \"p\",col=\"darkgreen\",cex=3,lwd=4)\n",
    "cor.test(dino$x,dino$y)\n",
    "\n",
    "# Repeat for the slant_down and slant_up datasets.\n",
    "slant_down <- datasaurus_dozen[datasaurus_dozen$dataset == 'slant_down',]\n",
    "plot(y ~ x, data=slant_down, type = \"p\",col=\"darkgreen\",cex=3,lwd=4)\n",
    "cor.test(slant_down$x,slant_down$y)\n",
    "\n",
    "slant_up <- datasaurus_dozen[datasaurus_dozen$dataset == 'slant_up',]\n",
    "plot(y ~ x, data=slant_up, type = \"p\",col=\"darkgreen\",cex=3,lwd=4)\n",
    "cor.test(slant_up$x,slant_up$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f5c5-9ec3-4c83-b635-60a29b1d8a62",
   "metadata": {},
   "source": [
    "#### Optional example 2\n",
    "\n",
    "Consider some crop yield data from Frossard 2019, in particular the number of ears of wheat and the number of ears with grains.\n",
    " - Load in the data and plot *nr.ears.m2* against *DW.ears_with_grains.g.m2*\n",
    " - Test if the above variables are correlated using a Pearson correlation test (`cor.test()`).\n",
    "   - What is the reported statistical significance?\n",
    "   - What is the r2 effect size?  \n",
    " - Repeat the two tasks above but using *nr.ears.m2* and *soil.cover.morning*. \\\n",
    "   Does the differences in correlation make sense based on your interpretation of these variables?\n",
    "\n",
    "**Extension**\n",
    " - What will sorting the data (like `ears <- sort(crop_data$nr.ears.m2)`) first do to the correlation coefficients and significance? \\\n",
    "   Is this a valid form of data preprocessing?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd63704-5504-46f6-943c-c4efd2dfe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data and plot it.\n",
    "crop_data <- read.table('Frossard_2019.csv',header=TRUE,sep=\",\")\n",
    "plot(DW.ears_with_grains.g.m2 ~ nr.ears.m2,data=crop_data, type = \"p\",col=\"darkgreen\",cex=3,lwd=4)\n",
    "\n",
    "# Let's calculate the correlation coefficient and p-value.\n",
    "# By default, it is the Pearson method.\n",
    "cor.test(crop_data$nr.ears.m2,crop_data$DW.ears_with_grains.g.m2)\n",
    "\n",
    "# We can also use the \"formula\" notation.\n",
    "# It is slightly different here, as \"~ Y + X\" rather than the \"Y ~ X\" we saw before\n",
    "#cor.test(~ DW.ears_with_grains.g.m2 + nr.ears.m2,data=crop_data)\n",
    "\n",
    "# And repeat for the *soil.cover.morning* variable\n",
    "plot(soil.cover.morning ~ nr.ears.m2,data=crop_data, type = \"p\",col=\"red\",cex=3,lwd=4)\n",
    "cor.test(crop_data$nr.ears.m2,crop_data$soil.cover.morning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe99df-d2da-472e-82c7-b3774bb88efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional example 2 extension\n",
    "# What will sorting the data (like ears <- sort(crop_data$nr.ears.m2)) first do to the correlation coefficients and significance?\n",
    "\n",
    "ears <- sort(crop_data$nr.ears.m2)\n",
    "weight <- sort(crop_data$DW.ears_with_grains.g.m2)\n",
    "plot(ears, weight, type = \"p\",col=\"royalblue\",cex=3,lwd=4)\n",
    "cor.test(ears,weight)\n",
    "\n",
    "# Sorting the data independently removes the relationship between the two variables, and so destroys the original correlation.\n",
    "# The plot now looks very correlated, as both variables were sorted separately, and indeed there is a strong and significant correlation.\n",
    "\n",
    "# Preprocessing data without careful thought can introduce or remove many important correlations, so be careful!\n",
    "# Instead, if we sort row by row, preserving the paired relationship, the plot looks the same as before.\n",
    "sorted_crop <- crop_data[order(crop_data$nr.ears.m2),]\n",
    "plot(DW.ears_with_grains.g.m2 ~ nr.ears.m2,data=sorted_crop, type = \"p\",col=\"darkgreen\",cex=3,lwd=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
