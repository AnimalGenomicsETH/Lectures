{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bd41b8-7726-44ca-8250-f4242e1405a2",
   "metadata": {},
   "source": [
    "# Statistical Tests I\n",
    "---\n",
    "\n",
    "Solutions are provided below.  \n",
    "Each example contains easier parts at the start, plus more challenging extensions.  \n",
    "The extensions are useful to understand the concepts more generally, and are likely close to the difficulty of the final parts of an exam question.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "Let's *test* if students choose seating in a non-random way. \\\n",
    "A null hypothesis is that students choose seats at random, so we would expect a 50/50 split between the left and the right of the classroom. \\\n",
    "However, let us consider 100 students, where 36 sit on the left and 64 sit on the right.\n",
    "\n",
    " - What test could we use to find the probability of that seating arrangement if there is an equal chance to sit in either location?\n",
    " - Create a 2x2 matrix and use Fisher's exact test instead. How does the significance compare? Try one- and two-sided tests.\n",
    " - Find the marginal sums for the matrix and convince yourself that the assumptions for using Fisher's exact test are satisfied.\n",
    "\n",
    "**Extension**\n",
    "\n",
    " - Does the sample size make much difference? Consider looking at the chi-square test (chisq).\n",
    " - How can we generalise to more than 2 categeories (first row of seats, second row of seats, third row of seats, etc.)?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6758e-729f-48fa-9855-25d084b17a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"Binomial test p =\",binom.test(64,100,.5)$p.value,\"\\n\\n\")\n",
    "\n",
    "# trivially this is the same as considering those who sat on the other side\n",
    "cat(\"Binomial test p =\",binom.test(36,100,.5)$p.value,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b38fba-9356-4882-a1c0-1b06c0083ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's now create a 2x2 \"observation\" matrix by reshaping a 4x1 list\n",
    "Seating <- matrix(c(36, 50, 64, 50),\n",
    "       nrow = 2,\n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Left\", \"Right\")))\n",
    "\n",
    "# View the matrix to check it is right\n",
    "print(Seating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47138f94-19ca-401c-a9ef-09fd7582b14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the test results.\n",
    "# Using `cat` allows us to print more complex statements with multiple parts (the initial text and then also the p value).\n",
    "# The `\\n` are special characters to add new lines and make the printing easier to read.\n",
    "\n",
    "result <- fisher.test(Seating)\n",
    "cat(\"\\nFisher's exact test (Two-sided)\\np =\",result$p,\"\\n\")\n",
    "\n",
    "# This is by default a two-sided test, checking if there is *a* difference.\n",
    "# What if the right side is closer to the door, so we think people might prefer that side?\n",
    "# We can also use a one-sided version to test if \"more people sit on the left than expected\" or \"less people sit on the left than expected\".\n",
    "\n",
    "result <- fisher.test(Seating, alternative=\"greater\")\n",
    "cat(\"\\nFisher's exact test (One-sided)\\np =\",result$p,\"\\n\")\n",
    "\n",
    "result <- fisher.test(Seating, alternative=\"less\")\n",
    "cat(\"\\nFisher's exact test (One-sided)\\np =\",result$p,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f629b-420a-47f8-9a51-995bf7c06588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the \"marginals\" in more detail.\n",
    "# addMargins sums these values for a matrix.\n",
    "\n",
    "print(addmargins(Seating))\n",
    "\n",
    "# Consider a slightly different seating arrangement.\n",
    "\n",
    "Seating <- matrix(c(45, 50, 55, 50),\n",
    "       nrow = 2,\n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Left\", \"Right\")))\n",
    "\n",
    "# Now look at the margins compared to before.\n",
    "# The sum over the columns (on the right side) are still equal to 100.\n",
    "# This should be fixed, we have 100 students no matter what (unless the lecture is that bad and they leave halfway through).\n",
    "# However, the sum over the rows can change depending on where students can sit, and so this is NOT fixed.\n",
    "\n",
    "cat(\"\\n\")\n",
    "print(addmargins(Seating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837cb4f-b643-4eba-b40a-a8353ca534e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1 extension\n",
    "\n",
    "# Does the sample size make much difference? Consider looking at the chi-square test (chisq).\n",
    "\n",
    "Seating <- matrix(c(360, 500, 640, 500),\n",
    "       nrow = 2,\n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Left\", \"Right\")))\n",
    "result <- fisher.test(Seating)\n",
    "cat(\"\\nFisher's exact test (Two-sided)\\np =\",result$p,\"\\n\")\n",
    "\n",
    "# Yes, larger sample size (but proportionally the same) is highly statistically significant. Smaller sample sizes are much harder to be significant.\n",
    "\n",
    "cat(\"\\nChi Square test (Two-sided)\\np =\",chisq.test(Seating)$p.value,\"\\n\")\n",
    "\n",
    "# The chi square test is very similar, but since it is non-parametric, it is less powerful than Fisher's exact test in this case. But it is also more general.\n",
    "\n",
    "# Can we generalise to more than 2 categeories (first row of seats, second row of seats, third row of seats, etc.)?\n",
    "# Make a larger matrix with a few extra value\n",
    "\n",
    "Seating <- matrix(c(10, 25, 40, 25,28,22, 30,20, 24,26, 15, 35),\n",
    "       nrow = 2,       \n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Row 1\", \"Row 2\", \"Row 3\",\"Row 4\", \"Row 5\",\"Row 6\")))\n",
    "\n",
    "# Chi square test works fine, but Fisher's exact test gives an error (as the exact test becomes too hard to calculate, it is mostly for 2x2 matrices).\n",
    "print(Seating)\n",
    "\n",
    "#cat(\"\\nChi Square test (Two-sided)\\np =\",chisq.test(Seating)$p.value,\"\\n\")\n",
    "cat(\"\\nFisher's exact test (Two-sided)\\np =\",fisher.test(Seating)$p,\"\\n\")\n",
    "\n",
    "# can approximate the larger Fisher test using \"simulate.p.value=TRUE\"\n",
    "#cat(\"\\nFisher's exact test (Two-sided)\\np =\",fisher.test(Seating,simulate.p.value=TRUE)$p,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbf279-5e70-45ad-b40e-453ced6252d4",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 2\n",
    "Let's consider (randomly generated) dairy yield from two different cattle breeds.\n",
    " - Generate two sets of normally distributed data, use the same sample size and std, but a different mean.\n",
    " - Plot the distributions using histograms.\n",
    " - Use an appropriate statistical test to see if the two cattle breeds have similar milk yields.\n",
    "\n",
    "**Extension**\n",
    " - Does changing var.equal to false in the statistical test change the results much?\n",
    " - What if we change the variances in the milk yields from 2 to something different for each class?\n",
    " - Can you find an example where a TRUE/FALSE value for var.equal would give a significant or not-significant result? What would that mean?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b4179-da80-4d9f-bd33-e890ea1ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 normally distributed yield variates, with a slightly different means but both with standard deviation of 2.\n",
    "\n",
    "holstein <- rnorm(1000,90,2)\n",
    "brown_swiss <- rnorm(1000,88,2)\n",
    "\n",
    "# Let's plot the distribution just to help visualise.\n",
    "# Plotting these as histograms with different colours in the same plot (add=T) and title (main=\"Exam results\").\n",
    "\n",
    "hist(holstein,col=rgb(0,0,1,1/4), xlim=c(80,100),main=\"Milk yield\",xlab=\"yield (L)\")\n",
    "hist(brown_swiss,col=rgb(1,0,0,1/4), xlim=c(80,100),add=T)\n",
    "\n",
    "# Manually add a legend to the plot for the two classes\n",
    "\n",
    "legend(\"topright\",c(\"Holstein\",\"Brown Swiss\"),fill=c(rgb(0,0,1,1/4),rgb(1,0,0,1/4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c5661-b9ae-41a8-a711-95831a0b4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our null hypothesis is that the two breeds have the same yield.\n",
    "# Can use the t-test, which assumes normally distributed data and equal variances.\n",
    "\n",
    "result <- t.test(holstein,brown_swiss,var.equal=T)\n",
    "cat(\"p =\",result$p.value,\"\\ndof =\",result$parameter,\"\\nt statistic =\",result$statistic,\"\\n\")\n",
    "\n",
    "# Given the degrees of freedom and the t statistic, we can also calculate the p-value from the theoretical Students t distribution.\n",
    "# Because we used a two-sided test, multiply the result by 2.\n",
    "\n",
    "cat(\"Distribution calculated p =\",2*pt(result$statistic,result$parameter, lower.tail=FALSE),\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f6101-75d4-441b-8fd3-5f0a29bbc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2 extension\n",
    "# Does changing var.equal in EXAMPLE 2 to false change the results much?\n",
    " \n",
    "holstein <- rnorm(1000,90,2)\n",
    "brown_swiss <- rnorm(1000,88,2)\n",
    "\n",
    "result <- t.test(holstein,brown_swiss,var.equal=F)\n",
    "cat(\"p =\",result$p.value,\"\\ndof =\",result$parameter,\"\\nt statistic =\",result$statistic,\"\\n\")\n",
    "\n",
    "#There is a tiny change in degrees of freedom when variance is not equal (it is now \"Welch's t-test\"), but no change to t statistic and p value is still significant.\n",
    "\n",
    "\n",
    "# What if we change the variances in the milk yield from 2 to something different for each class?\n",
    "\n",
    "holstein <- rnorm(1000,90,3)\n",
    "brown_swiss <- rnorm(1000,88,4)\n",
    "\n",
    "result <- t.test(holstein,brown_swiss,var.equal=F)\n",
    "cat(\"p =\",result$p.value,\"\\ndof =\",result$parameter,\"\\nt statistic =\",result$statistic,\"\\n\")\n",
    "\n",
    "# Still significant, but there is a larger drop in degrees of freedom and t-statistic, as the distribitions of milk yields are more likely to overlap with larger variance.\n",
    "\n",
    "# Can you find an example where a TRUE/FALSE value for var.equal would give a significant or not-significant result? What would that mean?\n",
    "\n",
    "# There will be some parameters and random seed that will do this (similar to the case in Example 1).\n",
    "# If the variance truly is equal, then picking var.equal=F and getting a non-significant result means you used an \"under-powered\" test, and incorrectly will say it is non-significant (false negative error).\n",
    "# Alternatively, picking var.equal=T when they are not might mean your test is \"over-powered\" and you are incorrectly reporting it is as significant (false positive error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc367a-1589-4ac7-a9dc-38f0ee4a4d9a",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 3\n",
    "\n",
    "Consider the provided crop harvest data at many different locations over two different years.\n",
    "```\n",
    "year_2022 <- c(180,161,188,151,178,160,190,172,118,164,222,217,155,165,129,193,210,191,176,205,170,167,189)\n",
    "year_2023 <- c(133,211,168,137,144,172,143,174,139,156,170,162,212,170,171,161,177,162,151,186,132,158,97)\n",
    "```\n",
    " - Plot histograms for the two years, as well as a scatterplot.\n",
    " - Test separately if the two years follow a normal distribution.\n",
    "\n",
    "\n",
    "**Extension**\n",
    "\n",
    "Consider if the crop yields from 2022 and 2023 were measured at the same set of locations, so the first yield from 2022 and 2023 are from the same field.  \n",
    "The yields are now *paired* and can be compared more directly rather than just a list of yields.\n",
    " - We can use a paired test for both the parametric and nonparametric tests. How does this affect the results? What is the interpretation?\n",
    " - How does the power of the tests relate to which of the four tests (parametric versus nonparametric and unpaired versus paired) are significant.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c3a6f-711a-4f2e-88e9-0613eb7a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2022 <- c(180,161,188,151,178,160,190,172,118,164,222,217,155,165,129,193,210,191,176,205,170,167,189)\n",
    "year_2023 <- c(133,211,168,137,144,172,143,174,139,156,170,162,212,170,171,161,177,162,151,186,132,158,97)\n",
    "\n",
    "# How do the distributions look like?\n",
    "# Wouldn't expect anything too obvious since the locations could all be quite different\n",
    "\n",
    "hist(year_2022,col=rgb(1,0,0,1/4),breaks = seq(50,250,length.out = 16),main=\"Crop yield\",xlab=\"yield (kg)\")\n",
    "hist(year_2023,col=rgb(0,0,1,1/4),breaks = seq(50,250,length.out = 16), add=T) \n",
    "legend(\"right\",c(\"2022\",\"2023\"),fill=c(rgb(0,0,1,1/4),rgb(1,0,0,1/4)))\n",
    "\n",
    "plot(year_2022,year_2023,col=rgb(0,0,1),xlab=\"2022\",ylab=\"2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd76f6b-b9e6-43db-bd18-0a4c8a2b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm it is appropriate to assume they are normal distributions\n",
    "cat(\"2022 p =\",shapiro.test(year_2022)$p,\"\\n\")\n",
    "cat(\"2023 p =\",shapiro.test(year_2023)$p,\"\\n\")\n",
    "\n",
    "# And let's look at their mean values\n",
    "cat(\"2022 mean =\",mean(year_2022),\"\\n\")\n",
    "cat(\"2023 mean =\",mean(year_2023),\"\\n\")\n",
    "\n",
    "# And finally let's test that there is a difference in these samples.\n",
    "cat(\"t-test=\",t.test(year_2022, year_2023, paired = FALSE,exact=FALSE)$p.value,\"\\n\")\n",
    "\n",
    "# Even though the distributions are normal (or at least we can reject that they are not normal), we can still use a non-parametric test.\n",
    "# Technically, the null hypothesis that the distributions are identical, and you would not expect to draw larger values from one population.\n",
    "cat(\"Mann–Whitney U test p =\",wilcox.test(year_2022, year_2023, paired = FALSE,exact=FALSE)$p.value,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187a1ae-6541-48f0-80c8-a0f2c90a441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 3 extension\n",
    "# Consider if the crop yields from 2022 and 2023 were measured at the same set of locations, so the first yield from 2022 and 2023 are from the same field.\n",
    "# The yields are now \"paired\" and can be compared more directly rather than just a list of yields.\n",
    "#\n",
    "# We can then use a paired wilcox test, but how does this affect the results? What is the interpretation?\n",
    "#\n",
    "\n",
    "cat(\"Mann–Whitney U test p =\",wilcox.test(year_2022, year_2023, paired = F,exact=F)$p.value,\"\\n\")\n",
    "cat(\"Mann–Whitney U test p =\",wilcox.test(year_2022, year_2023, paired = T,exact=F)$p.value,\"\\n\")\n",
    "\n",
    "# In this case, the test is still significant, but a lot weaker and the effect size is also lower (see lecture 6).\n",
    "# Before we were considering if one year had higher yield than the the other in general, but now we can also say one year yielded more crops from the same locations than the other year (so is easier to support a claim of \"better\").\n",
    "\n",
    "\n",
    "# What if we ignored the fact the data is not normally distributed, and used something like a paired t-test? How does that look? How does that relate\n",
    "# to the power of the test?\n",
    "\n",
    "cat(\"Student's t test (unpaired) p =\",t.test(year_2022, year_2023, paired = F)$p.value,\"\\n\")\n",
    "cat(\"Student's t test (paired) p =\",t.test(year_2022, year_2023, paired = T)$p.value,\"\\n\")\n",
    "\n",
    "# Interestingly the p-value is higher for unpaired and lower for paired.\n",
    "# In general, since this test assumption is not appropriate (not normally distributed and too small sample size to approximate normality), it may be over/under-powered, depending on where the t-distribution is being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b7b12-842b-431a-b1c1-2e29a141775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## EXAMPLE 1\n",
    "# Does the sample size make much difference? Consider looking at the chi-square test (chisq).\n",
    "\n",
    "Seating <- matrix(c(360, 500, 640, 500),\n",
    "       nrow = 2,\n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Left\", \"Right\")))\n",
    "result <- fisher.test(Seating)\n",
    "cat(\"\\nFisher's exact test (Two-sided)\\np =\",result$p,\"\\n\")\n",
    "\n",
    "#Yes, larger sample size (but proportionally the same) is highly statistically significant. Smaller sample sizes are much harder to be significant.\n",
    "\n",
    "cat(\"\\nChi Square test (Two-sided)\\np =\",chisq.test(Seating)$p.value,\"\\n\")\n",
    "\n",
    "# The chi square test is very similar, but since it is non-parametric, it is less powerful than Fisher's exact test in this case. But it is also more general.\n",
    "\n",
    "\n",
    "\n",
    "# Can we generalise to more than 2 categeories (first row of seats, second row of seats, third row of seats, etc.)?\n",
    "\n",
    "# Make a larger matrix with a few extra value\n",
    "\n",
    "Seating <- matrix(c(10, 25, 40, 25,28,22, 30,20, 24,26, 15, 35),\n",
    "       nrow = 2,       \n",
    "       dimnames = list(c(\"Observed\", \"Expectation\"),\n",
    "                       c(\"Row 1\", \"Row 2\", \"Row 3\",\"Row 4\", \"Row 5\",\"Row 6\")))\n",
    "\n",
    "# chi square test works fine, but Fisher's exact test gives an error (as the exact test becomes too hard to calculate, it is mostly for 2x2 matrices).\n",
    "print(Seating)\n",
    "cat(\"\\nChi Square test (Two-sided)\\np =\",chisq.test(Seating)$p.value,\"\\n\")\n",
    "cat(\"\\nFisher's exact test (Two-sided)\\np =\",fisher.test(Seating)$p,\"\\n\")\n",
    "\n",
    "# can approximate the larger Fisher test using \"simulate.p.value=TRUE\"\n",
    "#cat(\"\\nFisher's exact test (Two-sided)\\np =\",fisher.test(Seating,simulate.p.value=TRUE)$p,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa8657-18eb-412c-8cac-bf37c346bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2 extension\n",
    "# Does changing var.equal in EXAMPLE 2 to false change the results much?\n",
    " \n",
    "holstein <- rnorm(1000,90,2)\n",
    "brown_swiss <- rnorm(1000,88,2)\n",
    "\n",
    "result <- t.test(holstein,brown_swiss,var.equal=F)\n",
    "cat(\"p =\",result$p.value,\"\\ndof =\",result$parameter,\"\\nt statistic =\",result$statistic,\"\\n\")\n",
    "\n",
    "#There is a tiny change in degrees of freedom when variance is not equal (it is now \"Welch's t-test\"), but no change to t statistic and p value is still significant.\n",
    "\n",
    "\n",
    "# What if we change the variances in the milk yield from 2 to something different for each class?\n",
    "\n",
    "holstein <- rnorm(1000,90,3)\n",
    "brown_swiss <- rnorm(1000,88,4)\n",
    "\n",
    "result <- t.test(holstein,brown_swiss,var.equal=F)\n",
    "cat(\"p =\",result$p.value,\"\\ndof =\",result$parameter,\"\\nt statistic =\",result$statistic,\"\\n\")\n",
    "\n",
    "# Still significant, but there is a larger drop in degrees of freedom and t-statistic, as the distribitions of milk yields are more likely to overlap with larger variance.\n",
    "\n",
    "# Can you find an example where a TRUE/FALSE value for var.equal would give a significant or not-significant result? What would that mean?\n",
    "\n",
    "# There will be some parameters and random seed that will do this (similar to the case in Example 1).\n",
    "# If the variance truly is equal, then picking var.equal=F and getting a non-significant result means you used an \"under-powered\" test, and incorrectly will say it is non-significant (false negative error).\n",
    "# Alternatively, picking var.equal=T when they are not might mean your test is \"over-powered\" and you are incorrectly reporting it is as significant (false positive error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992e97a-11a1-4a58-9afa-768cbfc60cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXAMPLE 3\n",
    "# Does increasing/decreasing N or B make it more or less likely to get significant hits even after p value correction?\n",
    "#\n",
    "# The corrections are very robust, and so in general will never report any significant hits.\n",
    "# However, increasing N/B (so more flips per person) can result in a person getting a very signficant outlier, which may be significant after correction.\n",
    "# Similarly, decreasing N/B (so more people flipping less coins) means the correction factor is higher, so even less likely for a signficant result after correction.\n",
    "\n",
    "\n",
    "# What sort of conditions would lead to one method being too conservative/lenient. Is it meaningful if you have a significant hit after one correction\n",
    "# but not another?\n",
    "\n",
    "# If your probabilities are very unevenly distibuted (so a few highly signficiant values and a few non-signifant values), then the Bonferroni correction will be much stricter than B-H.\n",
    "# They are both valid choices, so there is not any meaningful conclusion if p-values are significant after one correction but not the other. \n",
    "# However, it is important not to change your method because you didn't like the outcome (p-value hacking!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7414d2c-538c-4f77-8abc-af7e836b93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXAMPLE 4\n",
    "# Consider if the crop yields from 2022 and 2023 were measured at the same set of locations, so the first yield from 2022 and 2023 are from the same field.\n",
    "# The yields are now \"paired\" and can be compared more directly rather than just a list of yields.\n",
    "#\n",
    "# We can then use a paired wilcox test, but how does this affect the results? What is the interpretation?\n",
    "#\n",
    "\n",
    "cat(\"Mann–Whitney U test p =\",wilcox.test(year_2022, year_2023, paired = F,exact=F)$p.value,\"\\n\")\n",
    "cat(\"Mann–Whitney U test p =\",wilcox.test(year_2022, year_2023, paired = T,exact=F)$p.value,\"\\n\")\n",
    "\n",
    "# In this case, the test is still significant, but a lot weaker and the effect size is also lower (see lecture 6).\n",
    "# Before we were considering if one year had higher yield than the the other in general, but now we can also say one year yielded more crops from the same locations than the other year (so is easier to support a claim of \"better\").\n",
    "\n",
    "\n",
    "# What if we ignored the fact the data is not normally distributed, and used something like a paired t-test? How does that look? How does that relate\n",
    "# to the power of the test?\n",
    "\n",
    "cat(\"Student's t test (unpaired) p =\",t.test(year_2022, year_2023, paired = F)$p.value,\"\\n\")\n",
    "cat(\"Student's t test (paired) p =\",t.test(year_2022, year_2023, paired = T)$p.value,\"\\n\")\n",
    "\n",
    "# Interestingly the p-value is higher for unpaired and lower for paired.\n",
    "# In general, since this test assumption is not appropriate (not normally distributed and too small sample size to approximate normality), it may be over/under-powered, depending on where the t-distribution is being tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577f61e-4131-4306-bcbd-20c2e5984bb7",
   "metadata": {},
   "source": [
    "---\n",
    "## Real world example\n",
    "\n",
    "This is a slightly more realistic case, where we are examining some [actual data](https://opendata.swiss/en/dataset/rinder-schlachtgewichte-von-kuhen).  \n",
    "The goal here is to manipulate the data into a useful shape, and then assess statistical significance.  \n",
    "These examples are harder than you would encounter in an exam and use more advanced data analysis techniques, but are useful to try.\n",
    "\n",
    "This data is monthly data for the total number of slaughtered cattle separated by beef and dairy classifications, but it is easier to work with annual averages.\n",
    " - Reshape the dataframe by summing (\"aggregating\") each month over each year and type of use.\n",
    " - Test for statistical significance if the total number of slaughtered beef cattle is higher than dairy cattle.\n",
    "   - Try both one- and two-sided tests. How does the p-value change between them for different tests? How does that relate to the symmetry of the distribution?\n",
    " - Many tests assume equal variance. Use an F-test to check if this assumption holds for this real data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e00046-c74f-4567-9d70-7da790a747db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the csv, which uses ';' as the separator rather than ','.\n",
    "# The first row of the file is also some download information rather than data, so we skip the first row.\n",
    "slaughter_weights <- read.csv('cattle-slaughterWeights.csv',sep=';',skip=1)\n",
    "print(head(slaughter_weights,n=10))\n",
    "\n",
    "# We now want to reshape the data to sum over the 12 months in a year for each TypeOfUse and Year.\n",
    "grouped_weights <- aggregate(cbind(tot_number_animals=slaughter_weights$tot_number_animals), by=list(TypeOfUse=slaughter_weights$TypeOfUse,Year=slaughter_weights$Year), FUN=sum)\n",
    "\n",
    "# We can also do this more compactly with the '~' formula syntax, which produces the same output.\n",
    "# This type of syntax is very common for linear regression/ANOVA as well as some plotting.\n",
    "grouped_weights <- aggregate(tot_number_animals ~ TypeOfUse + Year,data=slaughter_weights, FUN=sum)\n",
    "\n",
    "# Print out the new dataframe so we can see it.\n",
    "print(head(grouped_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0a739-3969-4cb5-81b9-d176bacefdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use a t-test to see if the total number of cattle slaughtered is statistically different by their type of use (beef or dairy).\n",
    "# The data for each type of use is from the same time periods, so we want to use a paired t-test.\n",
    "\n",
    "t.test(grouped_weights[grouped_weights$TypeOfUse == 'Beef',]$tot_number_animals,grouped_weights[grouped_weights$TypeOfUse == 'Dairy',]$tot_number_animals,paired=TRUE)\n",
    "\n",
    "# Or again using the more compact formula synatx.\n",
    "t.test(tot_number_animals ~ TypeOfUse, data = grouped_weights,paired=TRUE)\n",
    "\n",
    "# We can also explore the one-sided tests, where we can change default setting from \"alternative='two.sided'\" to \"alternative='greater'\" or \"alternative='less'\".\n",
    "t.test(tot_number_animals ~ TypeOfUse, data = grouped_weights,paired=TRUE,alternative='two.sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce5826-e6ee-4424-bea2-c3186274f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The t-test we were using implicitly assumes unequal variances in the two sets of samples (var.equal=FALSE).\n",
    "# Was this the correct assumption to make?\n",
    "\n",
    "var.test(slaughter_weights[slaughter_weights$TypeOfUse == 'Beef',]$std, slaughter_weights[slaughter_weights$TypeOfUse == 'Dairy',]$std)\n",
    "\n",
    "# Again with the ~ syntax.\n",
    "var.test(std ~ TypeOfUse, data=slaughter_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
